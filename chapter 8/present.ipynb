{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"font-size: 56px;\">8</h1>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"font-size: 45px;\">unittest and Refactoring</h1>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"direction: rtl; text-align: right; font-size: 18px; line-height: 1.8;\">\n",
    "    <p>\n",
    "        این فصل بر اهمیت تست‌های واحد (Unit Testing) و بازسازی کد (Refactoring) در بهبود کیفیت و قابلیت نگهداری نرم‌افزار تمرکز دارد. \n",
    "        تست‌های خودکار نقش کلیدی در موفقیت پروژه‌ها ایفا می‌کنند و به توسعه‌دهندگان امکان می‌دهند با اطمینان کد را تغییر داده و نسخه‌های بهتری ارائه دهند.\n",
    "    </p>\n",
    "    <h2 style=\"font-size: 20px;\">نکات کلیدی این فصل:</h2>\n",
    "    <ul>\n",
    "        <li>اهمیت تست‌های خودکار در موفقیت یک پروژه.</li>\n",
    "        <li>نقش تست‌های واحد در ارزیابی کیفیت کد.</li>\n",
    "        <li>آشنایی با ابزارها و فریم‌ورک‌هایی برای توسعه تست‌های خودکار، مانند <code>unittest</code> و <code>pytest</code>.</li>\n",
    "        <li>استفاده از تست‌ها برای درک بهتر مشکلات دامنه و مستندسازی کد.</li>\n",
    "        <li>مفاهیمی مانند توسعه مبتنی بر تست (Test-Driven Development - TDD).</li>\n",
    "    </ul>\n",
    "    <p>\n",
    "        این فصل همچنین به اصول مهندسی نرم‌افزار و ابزارهای موجود در کتابخانه استاندارد پایتون و بسته‌های خارجی برای پیاده‌سازی تست‌های خودکار می‌پردازد.\n",
    "    </p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Design principles and unit testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"direction: rtl; text-align: right; font-size: 18px; line-height: 1.8;\">\n",
    "    <h2>مقدمه‌ای بر تست واحد (Unit Testing)</h2>\n",
    "    <p>\n",
    "        در این بخش، ابتدا به تست واحد (Unit Testing) از دیدگاه مفهومی می‌پردازیم. \n",
    "        برخی اصول مهندسی نرم‌افزار (Software Engineering Principles) که در فصل قبلی مورد بحث قرار گرفت را بازبینی می‌کنیم تا دریابیم چگونه این اصول به کدنویسی تمیز (Clean Code) مرتبط هستند. \n",
    "        سپس به جزئیات بیشتری در مورد نحوه اجرای این مفاهیم در سطح کد (Code Level) و ابزارها (Tools) و فریم‌ورک‌ها (Frameworks) که می‌توان از آن‌ها استفاده کرد، خواهیم پرداخت.\n",
    "    </p>\n",
    "    <p>\n",
    "        در ابتدا، تعریف کوتاهی از تست واحد (Unit Testing) ارائه می‌کنیم. \n",
    "        تست‌های واحد (Unit Tests) کدی هستند که وظیفه اعتبارسنجی بخش‌های دیگر کد را بر عهده دارند. \n",
    "        برخلاف تصورات معمول، این تست‌ها صرفاً برای اعتبارسنجی \"هسته\" (Core) برنامه نیستند، \n",
    "        بلکه خود بخشی حیاتی و اصلی از نرم‌افزار (Software) محسوب می‌شوند و باید همانند منطق تجاری (Business Logic) با دقت و توجه یکسانی مورد بررسی قرار گیرند.\n",
    "    </p>\n",
    "    <p>\n",
    "        یک تست واحد (Unit Test) قطعه‌ای از کد است که بخش‌هایی از کد حاوی منطق تجاری (Business Logic) را وارد کرده و آن را اجرا می‌کند. \n",
    "        این تست با شبیه‌سازی چندین سناریو (Scenarios) و تضمین شرایط خاص، منطق را مورد بررسی قرار می‌دهد. \n",
    "        تست‌های واحد (Unit Tests) باید ویژگی‌های زیر را داشته باشند:\n",
    "    </p>\n",
    "    <ul>\n",
    "        <li>\n",
    "            <strong>ایزوله بودن (Isolation):</strong> \n",
    "            تست‌های واحد باید کاملاً مستقل از هر عامل خارجی (External Agent) باشند و تنها بر منطق تجاری (Business Logic) تمرکز کنند. \n",
    "            به همین دلیل، آن‌ها به پایگاه داده (Database) متصل نمی‌شوند، درخواست‌های HTTP ارسال نمی‌کنند و غیره. \n",
    "            همچنین تست‌ها نباید به یکدیگر وابسته باشند؛ هر تست باید بتواند به صورت مستقل و بدون تکیه بر وضعیت قبلی اجرا شود.\n",
    "        </li>\n",
    "        <li>\n",
    "            <strong>عملکرد بالا (Performance):</strong> \n",
    "            تست‌های واحد باید سریع اجرا شوند، زیرا قرار است چندین بار و به‌طور مکرر اجرا شوند.\n",
    "        </li>\n",
    "        <li>\n",
    "            <strong>تکرارپذیری (Repeatability):</strong> \n",
    "            تست‌های واحد باید وضعیت نرم‌افزار را به شکلی قابل پیش‌بینی ارزیابی کنند. \n",
    "            اگر تستی شکست بخورد، باید تا زمان رفع اشکال کد، شکست بخورد. \n",
    "            اگر تستی موفق شود و تغییری در کد ایجاد نشود، باید همچنان موفق باقی بماند.\n",
    "        </li>\n",
    "        <li>\n",
    "            <strong>خوداعتباری (Self-Validation):</strong> \n",
    "            نتیجه تست‌های واحد باید بدون نیاز به تفسیر اضافی مشخص باشد. نیازی به دخالت دستی برای درک نتیجه تست‌ها وجود ندارد. \n",
    "            هنگامی که ابزار فایل‌های ما را اجرا می‌کند، یک فرآیند Python آغاز می‌شود و تست‌های ما در آن اجرا می‌شوند. \n",
    "            اگر تست‌ها شکست بخورند، فرآیند با کد خطا (Error Code) خاتمه می‌یابد (در محیط Unix، این کد عددی به جز 0 خواهد بود). \n",
    "            استاندارد این است که ابزار برای هر تست موفق یک نقطه (.)، برای هر تست شکست‌خورده یک F، و برای هر استثنا (Exception) یک E نمایش دهد.\n",
    "        </li>\n",
    "    </ul>\n",
    "    <p>\n",
    "        در زبان Python، این به معنای ایجاد فایل‌های جدید با پسوند *.py است که تست‌های واحد (Unit Tests) در آن‌ها قرار می‌گیرند و توسط ابزاری خاص اجرا می‌شوند. \n",
    "        این فایل‌ها شامل دستورات <code>import</code> برای وارد کردن منطق تجاری (Business Logic) مورد نظر خواهند بود، \n",
    "        و داخل این فایل‌ها تست‌ها را برنامه‌ریزی می‌کنیم. سپس ابزاری، تست‌ها را جمع‌آوری و اجرا می‌کند و نتیجه را نمایش می‌دهد.\n",
    "    </p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A note about other forms of automated testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"direction: rtl; text-align: right; font-size: 18px; line-height: 1.8;\">\n",
    "    <h3>تست‌های واحد (Unit Tests):</h3>\n",
    "    <p>\n",
    "        این تست‌ها بخش‌های کوچک و خاصی از کد، مانند توابع یا متدها را بررسی می‌کنند و باید دقیق و جزئی باشند. \n",
    "        برای بررسی بخش‌های بزرگ‌تر، مانند کلاس‌ها، از مجموعه‌ای از تست‌های واحد (test suite) استفاده می‌شود.\n",
    "    </p>\n",
    "    <h3>تست‌های یکپارچه‌سازی (Integration Tests):</h3>\n",
    "    <p>\n",
    "        این تست‌ها تعامل چندین مؤلفه را با هم بررسی می‌کنند تا اطمینان حاصل شود که سیستم به‌صورت کلی درست کار می‌کند. \n",
    "        برخلاف تست‌های واحد، این تست‌ها نیاز به شبیه‌سازی محیط تولید دارند (مانند درخواست‌های HTTP و اتصال به پایگاه داده) اما با استفاده از ابزارهایی مانند Docker برای شبیه‌سازی وابستگی‌ها.\n",
    "    </p>\n",
    "    <h3>تست‌های پذیرش (Acceptance Tests):</h3>\n",
    "    <p>\n",
    "        این تست‌ها از دیدگاه کاربر، عملکرد سیستم را بررسی می‌کنند و موارد استفاده (use cases) را اجرا می‌کنند. \n",
    "        این تست‌ها زمان بیشتری نسبت به تست‌های واحد می‌برند و کمتر اجرا می‌شوند.\n",
    "    </p>\n",
    "    <h3>سرعت و تکرار تست‌ها:</h3>\n",
    "    <p>\n",
    "        تست‌های واحد سریع هستند و مرتباً در طول توسعه اجرا می‌شوند، در حالی که تست‌های یکپارچه‌سازی و پذیرش به دلیل زمان‌بر بودن، کمتر اجرا می‌شوند. \n",
    "        هدف، داشتن تعداد زیادی تست واحد و تعداد کمتری تست‌های پیچیده‌تر برای پوشش مواردی است که تست‌های واحد نمی‌توانند پوشش دهند.\n",
    "   \n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unit testing and agile software development"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"direction: rtl; text-align: right; font-size: 18px; line-height: 1.8;\">\n",
    "    <p>\n",
    "        تست‌های واحداهمیت زیادی (Unit Tests) اهمیت زیادی در توسعه مدرن نرم‌افزار دارند. هدف اصلی در این نوع توسعه، ارائه‌ی ارزش به‌صورت مداوم و سریع است. \n",
    "        دلیل این کار این است که هرچه بازخورد زودتر دریافت شود، تأثیر تغییرات کمتر و اصلاح آن‌ها آسان‌تر خواهد بود.\n",
    "    </p>\n",
    "    <h3>نکات کلیدی:</h3>\n",
    "    <ul>\n",
    "        <li>\n",
    "            تغییرپذیری نرم‌افزار:\n",
    "            نرم‌افزار باید قابل تغییر، انعطاف‌پذیر، و قابل گسترش باشد تا بتوان به تغییرات پاسخ داد. \n",
    "            حتی اگر کد با اصول SOLID نوشته شده باشد، تضمینی نیست که تغییرات جدید بدون مشکل باشند.\n",
    "        </li>\n",
    "        <li>\n",
    "            مشکل اعتماد به تغییرات:\n",
    "            هنگام ایجاد تغییرات، نمی‌توان بدون مدرک مطمئن بود که عملکرد قبلی نرم‌افزار حفظ شده و باگ جدیدی وارد نشده است.\n",
    "        </li>\n",
    "        <li>\n",
    "            نقش تست‌های واحد:\n",
    "            تست‌های واحد به عنوان یک مدرک رسمی عمل می‌کنند و نشان می‌دهند که نرم‌افزار طبق مشخصات کار می‌کند. \n",
    "            این تست‌ها به تیم اطمینان می‌دهند که می‌توانند بدون نگرانی از مشکلات احتمالی، تغییرات را اعمال کنند.\n",
    "        </li>\n",
    "        <li>\n",
    "            افزایش سرعت توسعه:\n",
    "            وجود تست‌های خوب باعث می‌شود تیم توسعه بتواند با سرعت بیشتری ارزش ارائه دهد، زیرا مشکلات کمتری در اثر باگ‌ها ایجاد می‌شود.\n",
    "        </li>\n",
    "    </ul>\n",
    "    <p>\n",
    "        به طور خلاصه، تست‌های واحد ابزار حیاتی برای اطمینان از کیفیت و سرعت در توسعه نرم‌افزار هستند.\n",
    "    </p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# unit testing and software design"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir=\"rtl\" style=\"text-align: right;\">\n",
    "این روی دیگر سکه در رابطه بین کد اصلی و آزمون واحد است. علاوه بر دلایل عملی که در بخش قبلی بررسی کردیم، این موضوع به این حقیقت بازمی‌گردد که نرم‌افزار خوب، نرم‌افزاری قابل آزمون است.  \n",
    "قابلیت آزمون (ویژگی‌ای که تعیین می‌کند نرم‌افزار چقدر آسان قابل آزمودن است) نه تنها یک ویژگی دلخواه بلکه یک عامل محرک برای کدنویسی تمیز است.  \n",
    "\n",
    "آزمون‌های واحد تنها چیزی تکمیلی برای کد اصلی نیستند، بلکه تأثیر مستقیم و واقعی بر نحوه نگارش کد دارند. این موضوع در سطوح مختلفی قابل مشاهده است؛ از همان ابتدا، وقتی متوجه می‌شویم که برای افزودن آزمون‌های واحد به برخی از بخش‌های کد خود، باید آن را تغییر دهیم (که منجر به نسخه بهتری از کد می‌شود)، تا بیان نهایی آن (که در پایان این فصل بررسی خواهد شد) که کل کد (طراحی) بر اساس نحوه آزمودنش هدایت می‌شود، از طریق طراحی مبتنی بر آزمون.  \n",
    "\n",
    "با یک مثال ساده شروع می‌کنیم. در اینجا یک مورد استفاده کوچک را نشان خواهم داد که در آن آزمون‌ها (و نیاز به آزمودن کد ما) منجر به بهبود در نحوه نگارش نهایی کد می‌شوند.  \n",
    "\n",
    "در مثال زیر، فرآیندی را شبیه‌سازی می‌کنیم که نیاز به ارسال معیارها به یک سیستم خارجی دارد تا نتایج به‌دست‌آمده از هر وظیفه خاص را گزارش دهد (همان‌طور که همیشه، جزئیات تأثیری ندارند، تا زمانی که روی کد تمرکز کنیم). ما یک شیء `Process` داریم که نمایانگر یک وظیفه در مسئله دامنه است و از یک مشتری معیارها (یک وابستگی خارجی و بنابراین چیزی که ما کنترلی روی آن نداریم) برای ارسال معیارها به موجودیت خارجی استفاده می‌کند (برای مثال این می‌تواند ارسال داده به syslog یا statsd باشد).  \n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetricsClient:\n",
    "    \"\"\"3rd-party metrics client\"\"\"\n",
    "\n",
    "    def send(self, metric_name, metric_value):\n",
    "        if not isinstance(metric_name, str):\n",
    "            raise TypeError(\"expected type str for metric_name\")\n",
    "        if not isinstance(metric_value, str):\n",
    "            raise TypeError(\"expected type str for metric_value\")\n",
    "        logger.info(\"sending %s = %s\", metric_name, metric_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Process:\n",
    "    def __init__(self):\n",
    "        self.client = MetricsClient()  # A 3rd-party metrics client\n",
    "\n",
    "    def process_iterations(self, n_iterations):\n",
    "        for i in range(n_iterations):\n",
    "            result = self.run_process()\n",
    "            self.client.send(f\"iteration.{i}\", str(result))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir=\"rtl\" style=\"text-align: right;\">\n",
    "در نسخه شبیه‌سازی‌شده از کلاینت شخص ثالث، شرطی قرار داده‌ایم که پارامترهای ارائه‌شده باید از نوع رشته (string) باشند. بنابراین، اگر نتیجه متد `run_process` یک رشته نباشد، انتظار داریم که عملیات با شکست مواجه شود، و در واقع چنین اتفاقی رخ می‌دهد.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "Traceback (most recent call last):\n",
    "...\n",
    "raise TypeError(\"expected type str for metric_value\")\n",
    "TypeError: expected type str for metric_value\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir=\"rtl\" style=\"text-align: right;\">\n",
    "به یاد داشته باشید که این اعتبارسنجی خارج از کنترل ماست و نمی‌توانیم کد آن را تغییر دهیم، بنابراین باید قبل از ادامه، مقادیر مناسب با نوع صحیح را به متد ارائه دهیم.  \n",
    "اما از آنجا که این یک باگ است که شناسایی کرده‌ایم، ابتدا می‌خواهیم یک آزمون واحد بنویسیم تا مطمئن شویم این مشکل دیگر رخ نمی‌دهد. این کار را برای اثبات رفع باگ انجام می‌دهیم و همچنین برای محافظت در برابر این باگ در آینده، بدون توجه به تعداد دفعات تغییر کد.  \n",
    "\n",
    "امکان آزمودن کد به همین شکل و با استفاده از شبیه‌سازی (mocking) کلاینت شیء **`Process`** وجود دارد (در بخش اشیاء شبیه‌سازی‌شده توضیح خواهیم داد که چگونه این کار را انجام دهیم)، اما چنین رویکردی منجر به اجرای بخش بیشتری از کد از آنچه مورد نیاز است می‌شود (توجه کنید که بخشی که می‌خواهیم آزمون کنیم در کد لانه‌گذاری شده است).  \n",
    "علاوه بر این، کوچک بودن متد نکته مثبتی است، چرا که اگر این‌گونه نبود، آزمون باید بخش‌های ناخواسته بیشتری را که ممکن است نیاز به شبیه‌سازی داشته باشند اجرا می‌کرد.  \n",
    "این موضوع نمونه دیگری از طراحی خوب (توابع یا متدهای کوچک و منسجم) است که با قابلیت آزمون‌پذیری مرتبط است.  \n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir=\"rtl\" style=\"text-align: right;\">\n",
    "در نهایت، تصمیم می‌گیریم که وارد دردسرهای زیاد نشویم و فقط بخشی را که نیاز داریم آزمون کنیم. بنابراین، به جای تعامل مستقیم با کلاینت در متد اصلی، این کار را به یک متد واسط (wrapper method) واگذار می‌کنیم. کلاس جدید به این شکل خواهد بود:\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetricsClient:\n",
    "    \"\"\"3rd-party metrics client\"\"\"\n",
    "    def send(self, metric_name, metric_value):\n",
    "        if not isinstance(metric_name, str):\n",
    "            raise TypeError(\"expected type str for metric_name\")\n",
    "        if not isinstance(metric_value, str):\n",
    "            raise TypeError(\"expected type str for metric_value\")\n",
    "        logger.info(\"sending %s = %s\", metric_name, metric_value)\n",
    "\n",
    "\n",
    "class WrappedClient:\n",
    "    def __init__(self):\n",
    "        self.client = MetricsClient()\n",
    "\n",
    "    def send(self, metric_name, metric_value):\n",
    "        return self.client.send(str(metric_name), str(metric_value))\n",
    "\n",
    "\n",
    "class Process:\n",
    "    def __init__(self):\n",
    "        self.client = WrappedClient()\n",
    "\n",
    "    def process_iterations(self, n_iterations):\n",
    "        for i in range(n_iterations):\n",
    "            result = self.run_process()\n",
    "            self.client.send(f\"iteration.{i}\", str(result))\n",
    "\n",
    "    def run_process(self):\n",
    "        # Simulate a process and return a result\n",
    "        return 42  # Example dummy result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir=\"rtl\" style=\"text-align: right;\">\n",
    "در این مورد، تصمیم گرفتیم نسخه‌ای از کلاینت برای متریک‌ها ایجاد کنیم که در واقع یک واسط (wrapper) برای کتابخانه شخص ثالثی است که قبلاً استفاده می‌کردیم. برای این کار، یک کلاس ایجاد می‌کنیم که (با همان رابط کاربری) تبدیل نوع داده‌ها را به درستی انجام می‌دهد.  \n",
    "\n",
    "این روش استفاده از ترکیب (composition) شباهت زیادی به الگوی طراحی **آداپتور (Adapter)** دارد (الگوهای طراحی را در فصل بعدی بررسی خواهیم کرد، بنابراین فعلاً این فقط یک پیام اطلاعاتی است)، و از آنجا که این یک شیء جدید در دامنه ماست، می‌تواند آزمون‌های واحد مخصوص به خود را داشته باشد.  \n",
    "\n",
    "داشتن این شیء تست کردن را ساده‌تر می‌کند، اما مهم‌تر از همه، اکنون که به آن نگاه می‌کنیم، متوجه می‌شویم که احتمالاً این همان روشی است که کد باید از ابتدا نوشته می‌شد. تلاش برای نوشتن یک آزمون واحد برای کد ما باعث شد متوجه شویم که کاملاً یک انتزاع مهم را نادیده گرفته بودیم!  \n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir=\"rtl\" style=\"text-align: right;\">\n",
    "حالا که متد را همان‌طور که باید جدا کرده‌ایم، بیایید آزمون واحد واقعی برای آن بنویسیم. جزئیات مربوط به ماژول **`unittest`** که در این مثال استفاده شده است، در بخشی از این فصل که به ابزارها و کتابخانه‌های تست می‌پردازد، با جزئیات بیشتری بررسی خواهد شد.  \n",
    "اما فعلاً، خواندن کد به ما یک برداشت اولیه از نحوه تست آن می‌دهد و مفاهیم قبلی را کمی کمتر انتزاعی می‌کند.  \n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unittest\n",
    "from unittest.mock import Mock\n",
    "\n",
    "class TestWrappedClient(unittest.TestCase):\n",
    "    def test_send_converts_types(self):\n",
    "        wrapped_client = WrappedClient()\n",
    "        wrapped_client.client = Mock()\n",
    "        wrapped_client.send(\"value\", 1)\n",
    "        wrapped_client.client.send.assert_called_with(\"value\", \"1\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ".\n",
    "----------------------------------------------------------------------\n",
    "\n",
    "Ran 1 test in 0.001s\n",
    "\n",
    "OK\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir=\"rtl\" style=\"text-align: right;\">\n",
    "**Mock** یک نوع داده در ماژول **`unittest.mock`** است که یک شیء بسیار کاربردی برای بررسی انواع رفتارها می‌باشد. به عنوان مثال، در این مورد از آن به جای کتابخانه شخص ثالث استفاده می‌کنیم (که در محدوده‌های سیستم شبیه‌سازی شده است، همان‌طور که در بخش بعدی توضیح داده شده است) تا بررسی کنیم که به درستی فراخوانی می‌شود (و بار دیگر، هدف ما تست کتابخانه نیست، بلکه اطمینان از صحت فراخوانی آن است). توجه کنید که چگونه یک فراخوانی مشابه آنچه در شیء **`Process`** انجام می‌شود، اجرا می‌کنیم، اما انتظار داریم که پارامترها به رشته تبدیل شوند.  \n",
    "\n",
    "این یک نمونه از تأثیر آزمون واحد در طراحی کد است: تلاش برای تست کردن کد، ما را به نسخه بهتری از آن رساند. حتی می‌توانیم فراتر برویم و بگوییم که این آزمون به اندازه کافی خوب نیست، به این دلیل که در خط دوم، تست واحد یک همکاری‌کننده داخلی از کلاینت واسط را بازنویسی می‌کند. برای رفع این مشکل، ممکن است بگوییم که کلاینت واقعی باید از طریق یک پارامتر ارائه شود (با استفاده از **تزریق وابستگی**)، به جای اینکه در متد سازنده ایجاد شود. و بار دیگر، آزمون واحد باعث شد به پیاده‌سازی بهتری فکر کنیم.  \n",
    "\n",
    "نتیجه این مثال این است که **آزمون‌پذیری** یک کد به کیفیت آن نیز اشاره دارد. به عبارت دیگر، اگر کدی سخت قابل آزمون باشد یا آزمون‌های آن پیچیده باشند، احتمالاً نیاز به بهبود دارد.  \n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir=\"rtl\" style=\"text-align: right; font-style: italic;\">\n",
    "«هیچ ترفندی برای نوشتن آزمون‌ها وجود ندارد؛ تنها ترفندهایی برای نوشتن کدی که قابل آزمون باشد وجود دارد.»  \n",
    "– میشکو هِوری\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining the boundaries of what to test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir=\"rtl\" style=\"text-align: right;\">\n",
    "تست کردن نیازمند تلاش است و اگر هنگام تصمیم‌گیری درباره‌ی آنچه باید تست شود دقت نکنیم، هرگز کار تست کردن به پایان نخواهد رسید. در نتیجه، مقدار زیادی تلاش بی‌هدف صرف می‌کنیم بدون اینکه دستاورد قابل‌توجهی داشته باشیم.\n",
    "\n",
    "ما باید تست کردن را به محدوده‌ی کد خود محدود کنیم. اگر این کار را انجام ندهیم، مجبور خواهیم شد وابستگی‌ها (کتابخانه‌ها یا ماژول‌های خارجی/شخص ثالث) در کد خود را نیز تست کنیم و سپس وابستگی‌های آن‌ها را، و این مسیر به شکلی بی‌پایان ادامه پیدا می‌کند.\n",
    "\n",
    "تست کردن وابستگی‌ها بر عهده‌ی ما نیست، بنابراین می‌توانیم فرض کنیم که این پروژه‌ها تست‌های خود را دارند. کافی است فقط بررسی کنیم که آیا فراخوانی‌های صحیح به وابستگی‌های خارجی با پارامترهای درست انجام می‌شوند یا نه (و این ممکن است حتی مورد قابل قبولی برای استفاده از *پچ کردن* باشد)، اما نباید بیش از این تلاش کنیم.\n",
    "\n",
    "این یکی دیگر از مواردی است که در آن طراحی خوب نرم‌افزار مؤثر واقع می‌شود. اگر در طراحی دقت کرده باشیم و مرزهای سیستم خود را به وضوح تعریف کرده باشیم (یعنی طراحی ما به سمت رابط‌ها باشد، نه پیاده‌سازی‌های مشخصی که تغییر خواهند کرد، و در نتیجه وابستگی به مؤلفه‌های خارجی را کاهش دهیم تا وابستگی زمانی را کم کنیم)، آنگاه ماژول‌های این رابط‌ها برای نوشتن تست‌های واحد بسیار آسان‌تر قابل شبیه‌سازی خواهند بود.\n",
    "\n",
    "در تست واحد خوب، ما باید روی مرزهای سیستم خود پچ کنیم و تمرکز خود را بر عملکرد اصلی‌ای که باید آزمایش شود قرار دهیم. ما کتابخانه‌های خارجی (ابزارهای شخص ثالث نصب شده از طریق *pip*، به عنوان مثال) را تست نمی‌کنیم، بلکه بررسی می‌کنیم که آیا این ابزارها به درستی فراخوانی شده‌اند یا خیر. وقتی در ادامه این فصل اشیاء شبیه‌سازی (*mock objects*) را بررسی کنیم، تکنیک‌ها و ابزارهایی برای انجام این نوع از بررسی‌ها معرفی خواهیم کرد.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tools for testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir=\"rtl\" style=\"text-align: right;\">\n",
    "ابزارهای زیادی برای نوشتن تست‌های واحد وجود دارند که هر یک مزایا و معایب خود را داشته و اهداف متفاوتی را دنبال می‌کنند. من دو کتابخانه‌ی رایج برای تست واحد در پایتون را معرفی می‌کنم. این کتابخانه‌ها اکثر (اگر نگوییم تمام) موارد استفاده را پوشش می‌دهند و بسیار محبوب هستند، بنابراین دانستن نحوه استفاده از آن‌ها مفید خواهد بود.\n",
    "\n",
    "علاوه بر فریم‌ورک‌های تست و کتابخانه‌های اجرای تست، معمولاً در پروژه‌ها تنظیماتی برای پوشش کد (Code Coverage) نیز یافت می‌شود که از آن به عنوان معیاری برای سنجش کیفیت استفاده می‌کنند. از آنجا که پوشش (وقتی به عنوان یک معیار استفاده شود) می‌تواند گمراه‌کننده باشد، پس از بررسی نحوه ایجاد تست‌های واحد، به این موضوع خواهیم پرداخت که چرا نباید آن را به سادگی پذیرفت.\n",
    "\n",
    "بخش بعدی با معرفی کتابخانه‌های اصلی که در این فصل برای تست واحد از آن‌ها استفاده خواهیم کرد، آغاز می‌شود.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir=\"rtl\" style=\"text-align: right;\">\n",
    "در این بخش، به بررسی دو فریم‌ورک برای نوشتن و اجرای تست‌های واحد می‌پردازیم.  \n",
    "اولی، **unittest**، در کتابخانه استاندارد پایتون موجود است، در حالی که دومی، **pytest**، باید به صورت خارجی از طریق **pip** نصب شود:  \n",
    "- **unittest**: [https://docs.python.org/3/library/unittest.html](https://docs.python.org/3/library/unittest.html)  \n",
    "- **pytest**: [https://docs.pytest.org/en/latest/](https://docs.pytest.org/en/latest/)  \n",
    "\n",
    "وقتی صحبت از پوشش سناریوهای تست برای کد می‌شود، **unittest** به تنهایی احتمالاً کافی خواهد بود، چرا که ابزارهای کمکی بسیاری دارد. با این حال، برای سیستم‌های پیچیده‌تر که دارای وابستگی‌های متعدد، ارتباط با سیستم‌های خارجی و شاید نیاز به پچ کردن اشیاء، تعریف فیکسچرها و پارامتردهی به تست‌ها هستند، **pytest** گزینه کامل‌تری به نظر می‌رسد.\n",
    "\n",
    "ما از یک برنامه کوچک به عنوان مثال استفاده خواهیم کرد تا نشان دهیم چگونه می‌توان آن را با استفاده از هر دو گزینه تست کرد، که در نهایت به ما کمک می‌کند تصویر بهتری از مقایسه این دو به دست آوریم.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir=\"rtl\" style=\"text-align: right;\">\n",
    "مثالی که ابزارهای تست را نشان می‌دهد، نسخه‌ای ساده‌شده از یک ابزار کنترل نسخه است که از بررسی کد در درخواست‌های ادغام پشتیبانی می‌کند. ما با معیارهای زیر شروع خواهیم کرد:  \n",
    "- یک درخواست ادغام رد می‌شود اگر حداقل یک نفر با تغییرات مخالف باشد.  \n",
    "- اگر هیچ‌کس مخالفت نکند و درخواست ادغام مورد تأیید حداقل دو توسعه‌دهنده دیگر باشد، تأیید می‌شود.  \n",
    "- در هر حالت دیگر، وضعیت آن در حالت معلق (pending) قرار می‌گیرد.  \n",
    "\n",
    "و در اینجا نمونه‌ای از کد ممکن آورده شده است:\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "\n",
    "class MergeRequestStatus(Enum):\n",
    "    APPROVED = \"approved\"\n",
    "    REJECTED = \"rejected\"\n",
    "    PENDING = \"pending\"\n",
    "\n",
    "class MergeRequest:\n",
    "    def __init__(self):\n",
    "        self._context = {\n",
    "            \"upvotes\": set(),\n",
    "            \"downvotes\": set(),\n",
    "        }\n",
    "\n",
    "    @property\n",
    "    def status(self):\n",
    "        if self._context[\"downvotes\"]:\n",
    "            return MergeRequestStatus.REJECTED\n",
    "        elif len(self._context[\"upvotes\"]) >= 2:\n",
    "            return MergeRequestStatus.APPROVED\n",
    "        return MergeRequestStatus.PENDING\n",
    "\n",
    "    def upvote(self, by_user):\n",
    "        self._context[\"downvotes\"].discard(by_user)\n",
    "        self._context[\"upvotes\"].add(by_user)\n",
    "\n",
    "    def downvote(self, by_user):\n",
    "        self._context[\"upvotes\"].discard(by_user)\n",
    "        self._context[\"downvotes\"].add(by_user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial status: pending\n",
      "Status after user1 upvotes: pending\n",
      "Status after user2 upvotes: approved\n",
      "Status after user3 downvotes: rejected\n",
      "Status after user3 changes to upvote: approved\n",
      "Upvotes: {'user2', 'user1', 'user3'}\n",
      "Downvotes: set()\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    mr = MergeRequest()\n",
    "\n",
    "    \n",
    "    print(\"Initial status:\", mr.status.value)  \n",
    "\n",
    "    # User1 upvotes\n",
    "    mr.upvote(\"user1\")\n",
    "    print(\"Status after user1 upvotes:\", mr.status.value)  \n",
    "\n",
    "    # User2 upvotes\n",
    "    mr.upvote(\"user2\")\n",
    "    print(\"Status after user2 upvotes:\", mr.status.value)  \n",
    "\n",
    "    # User3 downvotes\n",
    "    mr.downvote(\"user3\")\n",
    "    print(\"Status after user3 downvotes:\", mr.status.value)  \n",
    "\n",
    "    # User3 changes their vote to upvote\n",
    "    mr.upvote(\"user3\")\n",
    "    print(\"Status after user3 changes to upvote:\", mr.status.value)  \n",
    "\n",
    "    print(\"Upvotes:\", mr._context[\"upvotes\"])\n",
    "    print(\"Downvotes:\", mr._context[\"downvotes\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# unittest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Please open file \"unittest\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### If you want run this test, please open file \".\\unittest\\test_mr.py\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unittest\n",
    "\n",
    "class TestMergeRequestStatus(unittest.TestCase):\n",
    "    def test_simple_rejected(self):\n",
    "        merge_request = MergeRequest()\n",
    "        merge_request.downvote(\"maintainer\")\n",
    "        self.assertEqual(merge_request.status, MergeRequestStatus.REJECTED)\n",
    "\n",
    "    def test_just_created_is_pending(self):\n",
    "        self.assertEqual(MergeRequest().status, MergeRequestStatus.PENDING)\n",
    "\n",
    "    def test_pending_awaiting_review(self):\n",
    "        merge_request = MergeRequest()\n",
    "        merge_request.upvote(\"core-dev\")\n",
    "        self.assertEqual(merge_request.status, MergeRequestStatus.PENDING)\n",
    "\n",
    "    def test_approved(self):\n",
    "        merge_request = MergeRequest()\n",
    "        merge_request.upvote(\"dev1\")\n",
    "        merge_request.upvote(\"dev2\")\n",
    "        self.assertEqual(merge_request.status, MergeRequestStatus.APPROVED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exception management"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### If you want run this test, please open file \".\\unittest\\test_mr2.py\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MergeRequestException(Exception):\n",
    "    \"\"\"Custom exception for invalid operations on a MergeRequest.\"\"\"\n",
    "    pass\n",
    "\n",
    "class MergeRequestStatus:\n",
    "    OPEN = \"open\"\n",
    "    CLOSED = \"closed\"\n",
    "\n",
    "class MergeRequest:\n",
    "    def __init__(self):\n",
    "        self._context = {\n",
    "            \"upvotes\": set(),\n",
    "            \"downvotes\": set(),\n",
    "        }\n",
    "        self._status = MergeRequestStatus.OPEN\n",
    "\n",
    "    def close(self):\n",
    "        self._status = MergeRequestStatus.CLOSED\n",
    "\n",
    "    def _cannot_vote_if_closed(self):\n",
    "        if self._status == MergeRequestStatus.CLOSED:\n",
    "            raise MergeRequestException(\n",
    "                \"Can't vote on a closed merge request\"\n",
    "            )\n",
    "\n",
    "    def upvote(self, by_user):\n",
    "        self._cannot_vote_if_closed()\n",
    "        self._context[\"downvotes\"].discard(by_user)\n",
    "        self._context[\"upvotes\"].add(by_user)\n",
    "\n",
    "    def downvote(self, by_user):\n",
    "        self._cannot_vote_if_closed()\n",
    "        self._context[\"upvotes\"].discard(by_user)\n",
    "        self._context[\"downvotes\"].add(by_user)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parametrized test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### If you want run this test, please open file \".\\unittest\\test_mr_subTest.py\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MergeRequestException(Exception):\n",
    "    \"\"\"Custom exception for invalid operations on a MergeRequest.\"\"\"\n",
    "    pass\n",
    "\n",
    "class MergeRequestStatus:\n",
    "    OPEN = \"open\"\n",
    "    CLOSED = \"closed\"\n",
    "    APPROVED = \"approved\"\n",
    "    REJECTED = \"rejected\"\n",
    "    PENDING = \"pending\"\n",
    "\n",
    "class AcceptanceThreshold:\n",
    "    def __init__(self, merge_request_context: dict):\n",
    "        self._context = merge_request_context\n",
    "\n",
    "    def status(self):\n",
    "        if self._context[\"downvotes\"]:\n",
    "            return MergeRequestStatus.REJECTED\n",
    "        elif len(self._context[\"upvotes\"]) >= 2:\n",
    "            return MergeRequestStatus.APPROVED\n",
    "        return MergeRequestStatus.PENDING\n",
    "\n",
    "class MergeRequest:\n",
    "    def __init__(self):\n",
    "        self._context = {\n",
    "            \"upvotes\": set(),\n",
    "            \"downvotes\": set(),\n",
    "        }\n",
    "        self._status = MergeRequestStatus.OPEN\n",
    "\n",
    "    def close(self):\n",
    "        self._status = MergeRequestStatus.CLOSED\n",
    "\n",
    "    def upvote(self, by_user):\n",
    "        self._cannot_vote_if_closed()\n",
    "        self._context[\"downvotes\"].discard(by_user)\n",
    "        self._context[\"upvotes\"].add(by_user)\n",
    "\n",
    "    def downvote(self, by_user):\n",
    "        self._cannot_vote_if_closed()\n",
    "        self._context[\"upvotes\"].discard(by_user)\n",
    "        self._context[\"downvotes\"].add(by_user)\n",
    "\n",
    "    def _cannot_vote_if_closed(self):\n",
    "        if self._status == MergeRequestStatus.CLOSED:\n",
    "            raise MergeRequestException(\"Can't vote on a closed merge request\")\n",
    "\n",
    "    @property\n",
    "    def status(self):\n",
    "        if self._status == MergeRequestStatus.CLOSED:\n",
    "            return self._status\n",
    "        return AcceptanceThreshold(self._context).status()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unittest\n",
    "\n",
    "class TestAcceptanceThreshold(unittest.TestCase):\n",
    "    def test_approved_status(self):\n",
    "        context = {\n",
    "            \"upvotes\": {\"user1\", \"user2\"},\n",
    "            \"downvotes\": set()\n",
    "        }\n",
    "        threshold = AcceptanceThreshold(context)\n",
    "        self.assertEqual(threshold.status(), MergeRequestStatus.APPROVED)\n",
    "\n",
    "    def test_rejected_status(self):\n",
    "        context = {\n",
    "            \"upvotes\": {\"user1\"},\n",
    "            \"downvotes\": {\"user2\"}\n",
    "        }\n",
    "        threshold = AcceptanceThreshold(context)  \n",
    "        self.assertEqual(threshold.status(), MergeRequestStatus.REJECTED)\n",
    "\n",
    "    def test_pending_status(self):\n",
    "        context = {\n",
    "            \"upvotes\": {\"user1\"},\n",
    "            \"downvotes\": set()\n",
    "        }\n",
    "        threshold = AcceptanceThreshold(context)\n",
    "        self.assertEqual(threshold.status(), MergeRequestStatus.PENDING)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# parametrized test by using subTest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### If you want run this test, please open file \".\\unittest\\subTest_add_test.py\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unittest\n",
    "\n",
    "def my_function(number: int):\n",
    "    return \"odd\" if number % 2 == 0 else \"even\"\n",
    "\n",
    "class TestMyFunction(unittest.TestCase):\n",
    "    def test_eo(self):\n",
    "        test_cases = [\n",
    "            (2, \"odd\"),\n",
    "            (5, \"even\"),\n",
    "            (-2354, \"odd\"),\n",
    "        ]\n",
    "        \n",
    "        for number, expected in test_cases:\n",
    "            with self.subTest(number=number, expected=expected):\n",
    "                self.assertEqual(my_function(number), expected)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pytest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Basic test cases with pytest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### please open file \"pytest\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### If you want run this test, please open file \".\\pytest\\test_mr.py\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_simple_rejected():\n",
    "    merge_request = MergeRequest()\n",
    "    merge_request.downvote(\"maintainer\")\n",
    "    assert merge_request.status == MergeRequestStatus.REJECTED\n",
    "\n",
    "def test_just_created_is_pending():\n",
    "    assert MergeRequest().status == MergeRequestStatus.PENDING\n",
    "\n",
    "def test_pending_awaiting_review():\n",
    "    merge_request = MergeRequest()\n",
    "    merge_request.upvote(\"core-dev\")\n",
    "    assert merge_request.status == MergeRequestStatus.PENDING\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_invalid_types():\n",
    "    merge_request = MergeRequest()\n",
    "    pytest.raises(TypeError, merge_request.upvote, {\"invalid-object\"})\n",
    "\n",
    "def test_cannot_vote_on_closed_merge_request():\n",
    "    merge_request = MergeRequest()\n",
    "    merge_request.close()\n",
    "    pytest.raises(MergeRequestException, merge_request.upvote, \"dev1\")\n",
    "    with pytest.raises(\n",
    "        MergeRequestException,\n",
    "        match=\"can't vote on a closed merge request\",\n",
    "    ):\n",
    "        merge_request.downvote(\"dev1\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# parametrized test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### If you want run this test, please open file \".\\pytest\\test_parametrized.py\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@pytest.mark.parametrize(\"context,expected_status\", (\n",
    "    (\n",
    "        {\"downvotes\": set(), \"upvotes\": set()},\n",
    "        MergeRequestStatus.PENDING\n",
    "    ),\n",
    "    (\n",
    "        {\"downvotes\": set(), \"upvotes\": {\"dev1\"}},\n",
    "        MergeRequestStatus.PENDING,\n",
    "    ),\n",
    "    (\n",
    "        {\"downvotes\": \"dev1\", \"upvotes\": set()},\n",
    "        MergeRequestStatus.REJECTED,\n",
    "    ),\n",
    "    (\n",
    "        {\"downvotes\": set(), \"upvotes\": {\"dev1\", \"dev2\"}},\n",
    "        MergeRequestStatus.APPROVED,\n",
    "    ),\n",
    "),)\n",
    "def test_acceptance_threshold_status_resolution(context, expected_status):\n",
    "    assert AcceptanceThreshold(context).status() == expected_status"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# stacked paramerization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytest\n",
    "\n",
    "@pytest.mark.parametrize(\"x\", (1, 2))\n",
    "@pytest.mark.parametrize(\"y\", (\"a\", \"b\"))\n",
    "def my_test(x, y):\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# fixtures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### If you want run this test, please open file \".\\pytest\\test_mr_fixture.py\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@pytest.fixture\n",
    "def rejected_mr():\n",
    "    merge_request = MergeRequest()\n",
    "    merge_request.downvote(\"dev1\")\n",
    "    merge_request.upvote(\"dev2\")\n",
    "    merge_request.upvote(\"dev3\")\n",
    "    merge_request.downvote(\"dev4\")\n",
    "    return merge_request\n",
    "\n",
    "def test_simple_rejected(rejected_mr):\n",
    "    assert rejected_mr.status == MergeRequestStatus.REJECTED\n",
    "\n",
    "def test_rejected_with_approvals(rejected_mr):\n",
    "    rejected_mr.upvote(\"dev2\")\n",
    "    rejected_mr.upvote(\"dev3\")\n",
    "    assert rejected_mr.status == MergeRequestStatus.REJECTED\n",
    "\n",
    "def test_rejected_to_pending(rejected_mr):\n",
    "    rejected_mr.upvote(\"dev1\")\n",
    "    assert rejected_mr.status == MergeRequestStatus.PENDING\n",
    "\n",
    "def test_rejected_to_approved(rejected_mr):\n",
    "    rejected_mr.upvote(\"dev1\")\n",
    "    rejected_mr.upvote(\"dev2\")\n",
    "    assert rejected_mr.status == MergeRequestStatus.APPROVED\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# code coverage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir=\"rtl\" style=\"text-align: right; font-family: Tahoma, Arial, sans-serif;\">\n",
    "    <h2>یک متریک مفید در تست نرم‌افزار</h2>\n",
    "    <p>\n",
    "        که نشان می‌دهد چه خطوطی از کد هنگام اجرای تست‌ها اجرا شده‌اند. هدف این متریک کمک به شناسایی قسمت‌هایی از کد است که نیاز به تست دارند و همچنین بهبودهای احتمالی را در کد تولید (Production Code) و تست‌ها مشخص می‌کند.\n",
    "    </p>\n",
    "    <h3>کاربرد پوشش کد:</h3>\n",
    "    <ul>\n",
    "        <li>نشان دادن بخش‌هایی از کد که تحت پوشش تست‌ها نیستند.</li>\n",
    "        <li>اجبار به نوشتن تست برای کدهایی که بدون تست باقی مانده‌اند.</li>\n",
    "        <li>شناسایی بهبودهای لازم در کد تولید یا تست‌ها.</li>\n",
    "    </ul>\n",
    "    <h3>فرایند پوشش‌دهی کد:</h3>\n",
    "    <ul>\n",
    "        <li>ممکن است متوجه شویم که یک سناریوی تستی را کاملاً از قلم انداخته‌ایم.</li>\n",
    "        <li>نیاز به نوشتن تست‌های واحد بیشتر یا جامع‌تر پیدا می‌کنیم.</li>\n",
    "        <li>ممکن است کد تولید را ساده‌تر کرده و بخش‌های زائد یا غیرضروری را حذف کنیم.</li>\n",
    "        <li>گاهی متوجه می‌شویم که بخش‌هایی از کد غیرقابل دسترسی هستند و می‌توان آن‌ها را حذف کرد.</li>\n",
    "    </ul>\n",
    "    <h3>پوشش کد به عنوان یک متریک، نه هدف:</h3>\n",
    "    <ul>\n",
    "        <li>پوشش کد فقط یک ابزار اندازه‌گیری است و نباید رسیدن به 100% پوشش به عنوان هدف اصلی تلقی شود، زیرا این کار لزوماً منجر به کد سالم‌تر نمی‌شود.</li>\n",
    "        <li>معمولاً تعیین یک حداقل مثلاً 80% پوشش به عنوان مقدار مطلوب، رویکرد معقولی است.</li>\n",
    "        <li>توجه داشته باشید که اجرای یک خط کد به معنای تست کامل آن نیست؛ گاهی یک خط کد شامل شرایط منطقی مختلفی است که هر کدام نیاز به تست جداگانه دارند.</li>\n",
    "    </ul>\n",
    "    <h3>ابزار محبوب برای پوشش کد:</h3>\n",
    "    <p>یکی از ابزارهای رایج برای محاسبه پوشش کد، کتابخانه‌ی <a href=\"https://pypi.org/project/coverage/\">coverage</a> است.</p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up rest coverage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"direction: rtl; text-align: right; font-size: 18px; line-height: 1.8;\">\n",
    "    <p>\n",
    "        در مورد <code>pytest</code>، می‌توان بسته <strong>pytest-cov</strong> را نصب کرد. پس از نصب، هنگام اجرای تست‌ها باید به \n",
    "        <code>pytest</code> اطلاع دهیم که <code>pytest-cov</code> نیز اجرا شود و مشخص کنیم کدام بسته (یا بسته‌ها) باید پوشش داده شوند \n",
    "        (به همراه سایر پارامترها و تنظیمات).\n",
    "    </p>\n",
    "    <p>\n",
    "        این بسته از تنظیمات متعددی پشتیبانی می‌کند، از جمله انواع مختلف قالب‌های خروجی، و به راحتی می‌توان آن را با هر ابزار \n",
    "        <strong>CI</strong> ادغام کرد. اما در میان همه این ویژگی‌ها، یکی از گزینه‌های بسیار توصیه‌شده این است که پرچمی (flag) تنظیم شود که مشخص کند \n",
    "        کدام خطوط هنوز توسط تست‌ها پوشش داده نشده‌اند. این گزینه به ما کمک می‌کند تا کد خود را تحلیل کنیم و تست‌های بیشتری بنویسیم.\n",
    "    </p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "run\n",
    "\n",
    "pytest --cov=coverage_1 --cov-report=html --cov-report=term-missing test_coverage_1.py\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir=\"rtl\" style=\"text-align: right; font-family: Arial, sans-serif; line-height: 1.6;\">\n",
    "    <p>\n",
    "        اینجا به ما می‌گوید که خطی وجود دارد که تست واحد ندارد، بنابراین می‌توانیم نگاهی بیندازیم و ببینیم چگونه می‌توانیم برای آن یک تست واحد بنویسیم. \n",
    "        این یک سناریوی رایج است که متوجه می‌شویم برای پوشش دادن آن خطوط گم‌شده، باید کد را با ایجاد متدهای کوچک‌تر بازنویسی کنیم. \n",
    "        در نتیجه، کد ما بهتر به نظر می‌رسد، همانطور که در مثالی که در ابتدای این فصل دیدیم.\n",
    "    </p>\n",
    "    <p>\n",
    "        مشکل در وضعیت معکوس نهفته است—آیا می‌توانیم به پوشش بالا اعتماد کنیم؟ آیا این به معنای درست بودن کد ماست؟ \n",
    "        متأسفانه، داشتن پوشش تست خوب یک شرط ضروری ولی ناکافی برای کد تمیز است. \n",
    "        نداشتن تست برای بخش‌هایی از کد به وضوح چیزی بد است. \n",
    "        داشتن تست در واقع بسیار خوب است، اما این را فقط می‌توانیم برای تست‌هایی که وجود دارند بگوییم. \n",
    "        با این حال، ما اطلاعات زیادی در مورد تست‌هایی که نداریم، نداریم و ممکن است حتی زمانی که پوشش کد بالا است، شرایط زیادی را از دست داده باشیم.\n",
    "    </p>\n",
    "    <p>\n",
    "        این‌ها برخی از مشکلات پوشش تست هستند که در بخش بعدی به آن‌ها اشاره خواهیم کرد.\n",
    "    </p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Caveats of test coverage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir=\"rtl\" style=\"text-align: right; font-family: Arial, sans-serif; line-height: 1.6;\">\n",
    "    <p>\n",
    "        پوشش تست 100٪ به معنای کد بدون خطا نیست: داشتن پوشش تست 100٪ تضمین نمی‌کند که کد شما عاری از هر گونه خطا یا باگ باشد. \n",
    "        ممکن است تست‌ها تمام خطوط کد را اجرا کنند، اما ممکن است موارد مرزی، حالات خاص یا سناریوهای غیرمنتظره را پوشش ندهند.\n",
    "    </p>\n",
    "    <p>\n",
    "        پوشش تست بالا همیشه ضروری نیست: در برخی موارد، داشتن پوشش تست بسیار بالا ممکن است به اندازه کافی مفید نباشد. \n",
    "        به عنوان مثال، در برخی از سیستم‌های بسیار پیچیده، دستیابی به پوشش تست 100٪ ممکن است بسیار دشوار و پرهزینه باشد. \n",
    "        در این موارد، تمرکز بر روی پوشش تست در بخش‌های حیاتی و حساس‌تر ممکن است استراتژی بهتری باشد.\n",
    "    </p>\n",
    "    <p>\n",
    "        پوشش تست به تنهایی کافی نیست: پوشش تست یک معیار مهم برای کیفیت کد است، اما به تنهایی کافی نیست. \n",
    "        باید به سایر معیارهای کیفیت کد مانند خوانایی، قابلیت نگهداری و طراحی نیز توجه شود.\n",
    "    </p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_function(number: int):\n",
    "    return 'even ' if number % 2 == 0 else 'odd'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@pytest.mark.parametrize(\"number, excepted\", [('2', \"even\")])\n",
    "\n",
    "def test_add(number, expected):\n",
    "    assert my_function(number) == expected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align: right; direction: rtl; font-size: 16px;\">\n",
    "اگر تست‌ها را با استفاده از پوشش (Coverage) اجرا کنیم، گزارش به ما یک عدد جذاب ۱۰۰٪ برای پوشش کد می‌دهد. اما نیاز به گفتن نیست که ما نیمی از شرایط موجود در دستور تنها که اجرا شده است را تست نکرده‌ایم. حتی نگران‌کننده‌تر این است که، از آنجا که بخش <code>else</code> از دستور اجرا نشده است، نمی‌دانیم در چه شرایطی ممکن است کد ما خراب شود (برای اغراق بیشتر در این مثال، تصور کنید که به جای رشته <code>\"odd\"</code>، یک دستور نادرست مانند <code>1/0</code> وجود داشته باشد، یا اینکه یک فراخوانی به تابعی باشد). \n",
    "</p>\n",
    "\n",
    "<p style=\"text-align: right; direction: rtl; font-size: 16px;\">\n",
    "می‌توان گفت، ممکن است یک قدم جلوتر برویم و فکر کنیم که این تنها مسیر \"خوش‌بینانه\" است، زیرا ما مقادیر درستی به تابع ارائه می‌دهیم. اما در مورد انواع نادرست چه؟ تابع باید چگونه در برابر آن‌ها دفاع کند؟ \n",
    "</p>\n",
    "\n",
    "<p style=\"text-align: right; direction: rtl; font-size: 16px;\">\n",
    "همان‌طور که می‌بینید، حتی یک دستور ساده  ممکن است سوالات زیادی و شرایط تست متنوعی را ایجاد کند که باید برای آن‌ها آماده باشیم.\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir=\"rtl\" style=\"text-align: right; font-family: Arial, sans-serif; line-height: 1.6;\">\n",
    "    <p>\n",
    "        بررسی میزان پوشش کد ایده خوبی است و حتی می‌توان آستانه‌های پوشش کد را به عنوان بخشی از ساخت CI پیکربندی کرد، \n",
    "        اما باید در نظر داشته باشیم که این تنها یک ابزار دیگر برای ماست. \n",
    "        و درست مانند ابزارهای قبلی که بررسی کردیم (لینترها، بررسی‌کننده‌های کد، فرمت‌دهندگان و موارد مشابه)، \n",
    "        تنها در زمینه ابزارهای بیشتر و محیطی مناسب برای پایگاه کدی تمیز مفید است.\n",
    "    </p>\n",
    "    <p>\n",
    "        ابزار دیگری که در تلاش‌های تستی ما به ما کمک می‌کند، استفاده از اشیاء ساختگی (mock objects) است. \n",
    "        ما در بخش بعدی این موارد را بررسی می‌کنیم.\n",
    "    </p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# mock objects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Opent file \"mock\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: right; direction: rtl; font-size: 16px;\">\n",
    "    <p>\n",
    "        در مواردی، کد ما تنها چیزی نیست که در زمینه تست‌های ما حضور دارد. به هر حال، سیستم‌هایی که طراحی و پیاده‌سازی می‌کنیم باید کاری واقعی انجام دهند و این معمولاً به معنای اتصال به سرویس‌های خارجی است (مانند پایگاه‌های داده، سرویس‌های ذخیره‌سازی، APIهای خارجی، سرویس‌های ابری و غیره). از آنجا که این سیستم‌ها نیاز به اثرات جانبی دارند، این امر اجتناب‌ناپذیر است. هرچقدر هم که کد خود را انتزاعی کنیم، به سمت رابط‌ها برنامه‌نویسی کنیم و کد را از عوامل خارجی جدا کنیم تا اثرات جانبی را به حداقل برسانیم، باز هم این اثرات در تست‌های ما حضور خواهند داشت و نیازمند روشی مؤثر برای مدیریت آنها هستیم.\n",
    "    </p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: right; direction: rtl; font-size: 16px;\">\n",
    "    <p>\n",
    "        اشیای شبیه‌سازی‌شده (Mock Objects) یکی از بهترین تاکتیک‌ها برای محافظت از تست‌های واحد ما در برابر اثرات جانبی نامطلوب هستند (همان‌طور که قبلاً در این فصل اشاره شد). ممکن است کد ما نیاز به انجام یک درخواست HTTP یا ارسال ایمیل داشته باشد، اما ما به‌طور قطع نمی‌خواهیم این کارها در تست‌های واحد انجام شود. تست‌های واحد باید فقط روی منطق کد ما تمرکز کنند و سریع اجرا شوند، زیرا می‌خواهیم آن‌ها را مرتباً اجرا کنیم، و این به این معناست که نمی‌توانیم تأخیر را تحمل کنیم. بنابراین، تست‌های واقعی واحد هیچ سرویسی واقعی را استفاده نمی‌کنند—به هیچ پایگاه داده‌ای متصل نمی‌شوند، درخواست HTTP نمی‌فرستند و اساساً هیچ کاری به جز بررسی منطق کد تولیدی انجام نمی‌دهند.\n",
    "    </p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: right; direction: rtl; font-size: 16px;\">\n",
    "    <p>\n",
    "        ما به تست‌هایی نیاز داریم که چنین کارهایی انجام دهند، اما آن‌ها تست‌های واحد نیستند. تست‌های یکپارچه (Integration Tests) برای بررسی عملکرد از دیدی وسیع‌تر طراحی شده‌اند و تقریباً رفتار یک کاربر را شبیه‌سازی می‌کنند. اما این تست‌ها سریع نیستند. از آنجا که به سیستم‌ها و سرویس‌های خارجی متصل می‌شوند، زمان بیشتری می‌برند و اجرای آن‌ها پرهزینه‌تر است. به‌طور کلی، ما می‌خواهیم تعداد زیادی تست واحد داشته باشیم که سریع اجرا شوند تا بتوانیم آن‌ها را همیشه اجرا کنیم، و تست‌های یکپارچه را کمتر اجرا کنیم (مثلاً هنگام ارسال یک درخواست ادغام جدید).\n",
    "    </p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: right; direction: rtl; font-size: 16px;\">\n",
    "    <p>\n",
    "        درحالی‌که اشیای شبیه‌سازی‌شده مفید هستند، استفاده بیش‌ازحد از آن‌ها می‌تواند به یک بوی بد کد یا حتی یک الگوی ضدمفید (Anti-pattern) تبدیل شود. این اولین موضوعی است که در بخش بعدی قبل از ورود به جزئیات استفاده از اشیای شبیه‌سازی‌شده مورد بحث قرار می‌دهیم.\n",
    "    </p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align: right; direction: rtl; font-size: 16px;\">\n",
    "<strong> اشیای شبیه‌سازی‌شده (Mock Objects)</strong>\n",
    "</p>\n",
    "\n",
    "<p style=\"text-align: right; direction: rtl; font-size: 16px;\">\n",
    "1. <strong>ایده اصلی:</strong><br>\n",
    "هنگام نوشتن تست‌ها، کد ما ممکن است با سرویس‌های خارجی مانند پایگاه‌داده‌ها، API‌ها، یا سرویس‌های ابری ارتباط داشته باشد. اما در تست‌های واحد نمی‌خواهیم این ارتباطات واقعی رخ دهد. دلیل این امر سرعت و دقت تست‌ها است. تست‌های واحد باید سریع باشند و فقط منطق کد ما را بررسی کنند.\n",
    "</p>\n",
    "\n",
    "<p style=\"text-align: right; direction: rtl; font-size: 16px;\">\n",
    "2. <strong>استفاده از Mock Objects:</strong><br>\n",
    "اشیای شبیه‌سازی‌شده (Mocks) برای جایگزینی سرویس‌های خارجی در تست‌ها استفاده می‌شوند. به عنوان مثال، اگر کد شما باید یک ایمیل ارسال کند، Mock به جای ایمیل واقعی عمل می‌کند و فقط تأیید می‌کند که ایمیل \"ارسال\" شده است. تست‌های واحد واقعی هیچ سرویس واقعی را استفاده نمی‌کنند و فقط منطق کد را اجرا می‌کنند.\n",
    "</p>\n",
    "\n",
    "<p style=\"text-align: right; direction: rtl; font-size: 16px;\">\n",
    "3. <strong>تست‌های یکپارچه:</strong><br>\n",
    "تست‌های یکپارچه (Integration Tests) برای بررسی تعامل بین کد و سرویس‌های خارجی طراحی شده‌اند. اما این تست‌ها کندتر هستند و باید کمتر اجرا شوند.\n",
    "</p>\n",
    "\n",
    "<p style=\"text-align: right; direction: rtl; font-size: 16px;\">\n",
    "4. <strong>هشدار در مورد استفاده زیاد از Mocks:</strong><br>\n",
    "اگر بیش از حد از Mocks استفاده کنید، این ممکن است نشانه‌ای از یک طراحی نادرست کد باشد. بهتر است در این موارد ساختار کد را بهبود دهید.\n",
    "</p>\n",
    "\n",
    "<hr>\n",
    "\n",
    "<p style=\"text-align: right; direction: rtl; font-size: 16px;\">\n",
    "<strong> هشدار درباره استفاده از Mock</strong>\n",
    "</p>\n",
    "\n",
    "<p style=\"text-align: right; direction: rtl; font-size: 16px;\">\n",
    " <strong>تشخیص مشکلات کد با تست‌ها:</strong><br>\n",
    "تست‌ها می‌توانند به ما کمک کنند تا مشکلات طراحی کد (مانند کدهای غیرتمیز یا غیرقابل تست) را شناسایی کنیم. اگر مجبور باشید برای نوشتن یک تست ساده تغییرات زیادی انجام دهید، ممکن است کد شما نیاز به بازنویسی داشته باشد.\n",
    "</p>\n",
    "\n",
    "<p style=\"text-align: right; direction: rtl; font-size: 16px;\">\n",
    " <strong>Patching (پچ کردن):</strong><br>\n",
    "پچ کردن به این معناست که یک قسمت از کد واقعی را با یک نسخه شبیه‌سازی‌شده جایگزین کنید. این کار به کمک ابزارهایی مانند <code>unittest.mock.patch</code> انجام می‌شود. مشکل پچ کردن این است که ارتباط ما با کد واقعی از دست می‌رود و ممکن است باعث مشکلات عملکردی شود.\n",
    "</p>\n",
    "\n",
    "<p style=\"text-align: right; direction: rtl; font-size: 16px;\">\n",
    " <strong>استفاده از Mocks:</strong><br>\n",
    "استفاده از Mock در تست‌ها می‌تواند مفید باشد، اما اگر به درستی استفاده نشود، نشانه‌ای از وابستگی زیاد به سرویس‌های خارجی است.\n",
    "</p>\n",
    "\n",
    "<hr>\n",
    "\n",
    "\n",
    "\n",
    "<p style=\"text-align: right; direction: rtl; font-size: 16px;\">\n",
    " <strong>اشیای Mock:</strong><br>\n",
    "Mock نوعی شیء است که می‌تواند رفتارهای مشخصی را شبیه‌سازی کند. به عنوان مثال، می‌توانید به Mock بگویید وقتی تابعی خاص فراخوانی شد، چه مقداری برگرداند. Mock تمام اطلاعات مربوط به اینکه چه زمانی و چگونه فراخوانی شده است (مثل پارامترها و تعداد دفعات فراخوانی) را ذخیره می‌کند.\n",
    "</p>\n",
    "\n",
    "<p style=\"text-align: right; direction: rtl; font-size: 16px;\">\n",
    " <strong>ابزار <code>unittest.mock.Mock</code>:</strong><br>\n",
    "در پایتون، کتابخانه استاندارد unittest یک شیء Mock ارائه می‌دهد که می‌تواند برای تست رفتارهای مختلف استفاده شود. این ابزار به شما امکان می‌دهد بررسی کنید که آیا یک متد فراخوانی شده است، چند بار فراخوانی شده، و با چه پارامترهایی فراخوانی شده است.\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GitBranch:\n",
    "    def __init__(self, commits):\n",
    "        self._commits = {c[\"id\"]: c for c in commits}\n",
    "\n",
    "    def __getitem__(self, commit_id):\n",
    "        return self._commits[commit_id]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._commits)\n",
    "\n",
    "def author_by_id(commit_id, branch):\n",
    "    return branch[commit_id][\"author\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "commits = [\n",
    "    {\"id\": \"101\", \"author\": \"dev1\", \"message\": \"Initial commit\"},\n",
    "    {\"id\": \"102\", \"author\": \"dev2\", \"message\": \"Add feature\"}\n",
    "]\n",
    "branch = GitBranch(commits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{\n",
    "    \"101\": {\"id\": \"101\", \"author\": \"dev1\", \"message\": \"Initial commit\"},<br>\n",
    "    \"102\": {\"id\": \"102\", \"author\": \"dev2\", \"message\": \"Add feature\"}\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '101', 'author': 'dev1', 'message': 'Initial commit'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "branch[\"101\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(branch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(branch['102'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dev2'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "author_by_id('102', branch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### If you want run this test, please open file \".\\mock\\test_mock_1.py\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unittest.mock import Mock, MagicMock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_find_commit():\n",
    "    branch = GitBranch([{\"id\": \"123\", \"author\": \"dev2\"}])\n",
    "    assert author_by_id(\"123\", branch) == \"dev1\"\n",
    "    \n",
    "def test_find_any():\n",
    "    author = author_by_id(\"123\", Mock()) is not None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Traceback (most recent call last):\n",
    "  File \"<stdin>\", line 1, in <module>\n",
    "  File \"<stdin>\", line 2, in author_by_id\n",
    "TypeError: 'Mock' object is not subscriptable\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MagicMock"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### If you want run this test, please open file \".\\mock\\test_mock_1.py\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_find_any():\n",
    "    mbranch = MagicMock()\n",
    "    mbranch.__getitem__.return_value = {\"author\": \"test\"}\n",
    "    assert author_by_id(\"123\", mbranch) == \"test\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# a usecase for test doubles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### If you want run this test, please open file \".\\mock\\test_mock_2.py\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "import requests\n",
    "from constants import STATUS_ENDPOINT\n",
    "\n",
    "\n",
    "class BuildStatus:\n",
    "    \"\"\"The CI status of a pull request.\"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def build_date() -> str:\n",
    "        return datetime.utcnow().isoformat()\n",
    "\n",
    "    @classmethod\n",
    "    def notify(cls, merge_request_id, status):\n",
    "        build_status = {\n",
    "            \"id\": merge_request_id,\n",
    "            \"status\": status,\n",
    "            \"built_at\": cls.build_date(),\n",
    "        }\n",
    "        response = requests.post(STATUS_ENDPOINT, json=build_status)\n",
    "        response.raise_for_status()\n",
    "        return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unittest import mock\n",
    "\n",
    "@mock.patch(\"mock_2.requests\")\n",
    "def test_build_notification_sent(mock_requests):\n",
    "    build_date = \"2018-01-01T00:00:01\"\n",
    "    with mock.patch(\"mock_2.BuildStatus.build_date\", return_value=build_date):\n",
    "        BuildStatus.notify(123, \"OK\")\n",
    "\n",
    "    expected_payload = {\"id\": 123, \"status\": \"OK\", \"built_at\": build_date}\n",
    "    mock_requests.post.assert_called_with(\n",
    "        STATUS_ENDPOINT, json=expected_payload\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir=\"rtl\" style=\"text-align: right; font-family: Arial, sans-serif; line-height: 1.6;\">\n",
    "    <p>\n",
    "        مشکل کدهای وابسته و پیچیدگی تست‌ها: \n",
    "        هرچند mock کردن وابستگی‌های خارجی مفید است، اما اگر نیاز به patch کردن بیش از حد وجود داشته باشد (در نسبت به تعداد خطوط اصلی کد)، \n",
    "        می‌تواند نشان‌دهنده یک مشکل طراحی باشد (معروف به کد بدبو یا \"code smell\").\n",
    "    </p>\n",
    "    <p>\n",
    "        در این حالت، ممکن است لازم باشد کد بازنگری شود تا وابستگی‌ها به درستی جدا شوند و کد به‌صورت تمیزتر و قابل تست‌تر طراحی شود.\n",
    "    </p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir=\"rtl\" style=\"text-align: right; font-family: Arial, sans-serif; line-height: 1.8;\">\n",
    "    <h1>بازنویسی (Refactoring)</h1>\n",
    "    <p>\n",
    "        بازنویسی به معنای تغییر ساختار کد از طریق بازآرایی و تغییر نمای داخلی آن، \n",
    "        بدون ایجاد تغییر در رفتار خارجی آن است.\n",
    "    </p>\n",
    "    <p>\n",
    "        مثالی برای این موضوع این است که اگر کلاسی دارای مسئولیت‌های زیادی باشد و \n",
    "        متدهای بسیار طولانی داشته باشد، تصمیم بگیرید آن را تغییر دهید. این کار ممکن است \n",
    "        با استفاده از متدهای کوچک‌تر، ایجاد همکاران داخلی جدید، و توزیع مسئولیت‌ها به \n",
    "        اشیاء جدید و کوچک‌تر انجام شود. در طول این فرآیند، مراقب هستید که رابط اصلی \n",
    "        (interface) آن کلاس را تغییر ندهید، تمام متدهای عمومی آن را همان‌طور که بودند \n",
    "        حفظ کنید، و هیچ تغییری در امضا (signature) آن‌ها ایجاد نکنید. برای یک ناظر خارجی، \n",
    "        ممکن است این تغییرات قابل مشاهده نباشند (اما ما می‌دانیم که تغییرات انجام شده است).\n",
    "    </p>\n",
    "    <p>\n",
    "        بازنویسی یک فعالیت حیاتی در نگهداری نرم‌افزار است، اما بدون وجود تست‌های واحد نمی‌توان \n",
    "        آن را به درستی انجام داد. دلیل آن این است که با هر تغییری که انجام می‌شود، باید بدانیم \n",
    "        که کد ما همچنان درست کار می‌کند. به نوعی، می‌توان گفت که تست‌های واحد ما به عنوان \n",
    "        \"ناظر خارجی\" برای کد عمل می‌کنند و اطمینان می‌دهند که قراردادهای تعریف‌شده تغییر نکرده‌اند.\n",
    "    </p>\n",
    "    <h3>چرا نیاز به بازنویسی داریم؟</h3>\n",
    "    <p>\n",
    "        گاهی اوقات نیاز داریم که از نرم‌افزار برای ویژگی‌های جدید پشتیبانی کنیم یا از آن به روش‌هایی \n",
    "        استفاده کنیم که قبلاً پیش‌بینی نشده بودند. تنها راه تطبیق با این نیازها، بازنویسی کد است تا \n",
    "        انعطاف‌پذیرتر یا عمومی‌تر شود.\n",
    "    </p>\n",
    "    <h3>هدف بازنویسی</h3>\n",
    "    <p>\n",
    "        معمولاً هدف از بازنویسی این است که:\n",
    "        <ul>\n",
    "            <li>ساختار کد را بهبود ببخشیم.</li>\n",
    "            <li>کد را قابل خواندن‌تر و انعطاف‌پذیرتر کنیم.</li>\n",
    "        </ul>\n",
    "        چالش اصلی این است که همه این اهداف باید در حالی محقق شوند که عملکرد اصلی کد کاملاً حفظ شود.\n",
    "    </p>\n",
    "    <h3>نقش تست‌های رگرسیون و تست‌های واحد</h3>\n",
    "    <p>\n",
    "        محدودیت بازنویسی این است که باید کد همان عملکرد قبلی خود را حفظ کند، اما با نسخه‌ای جدید از کد.\n",
    "        برای اطمینان از حفظ عملکرد، باید تست‌های رگرسیون روی کد تغییر یافته اجرا شوند. \n",
    "        تنها راه مقرون‌به‌صرفه برای اجرای تست‌های رگرسیون، استفاده از تست‌های خودکار است.\n",
    "        <strong>تست‌های واحد</strong> بهترین نسخه از تست‌های خودکار هستند که هزینه‌ی کمی دارند و به ما کمک می‌کنند \n",
    "        تا از صحت عملکرد کد اطمینان حاصل کنیم.\n",
    "    </p>\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir=\"rtl\" style=\"text-align: right; font-family: Arial, sans-serif; line-height: 1.6;\">\n",
    "    <h3>اصل وارونگی وابستگی چیست؟</h3>\n",
    "    <p>\n",
    "        اصل وارونگی وابستگی یکی از اصول <strong>SOLID</strong> در طراحی شیءگرا است. این اصل بیان می‌کند که:\n",
    "    </p>\n",
    "    <ul>\n",
    "        <li><strong>ماژول‌های سطح بالا (High-Level Modules)</strong> نباید به <strong>ماژول‌های سطح پایین (Low-Level Modules)</strong> وابسته باشند؛ بلکه هر دو باید به یک <strong>رابط (Interface)</strong> وابسته باشند.</li>\n",
    "        <li>جزئیات پیاده‌سازی (<em>Implementation Details</em>) باید به انتزاعات (<em>Abstractions</em>) وابسته باشند، نه برعکس.</li>\n",
    "    </ul>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evolving our code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir=\"rtl\" style=\"text-align: right; font-family: Arial, sans-serif; line-height: 1.6;\">\n",
    "    <p>\n",
    "        در مثال قبلی، ما توانستیم اثرات جانبی را از کد خود جدا کنیم تا قابل تست شود، با این روش که بخش‌هایی از کد که به چیزهایی وابسته بودند که نمی‌توانستیم در تست واحد کنترل کنیم، <strong>patch</strong> کردیم. این یک روش خوب است، زیرا در نهایت تابع <code>mock.patch</code> برای چنین وظایفی بسیار مفید است و اشیائی که به آن معرفی می‌کنیم را جایگزین کرده و یک شیء Mock به ما برمی‌گرداند.\n",
    "    </p>\n",
    "    <p>\n",
    "        نقطه ضعف این روش این است که باید مسیر شیئی که قصد Mock کردن آن را داریم، از جمله ماژول، به صورت یک رشته مشخص کنیم. این موضوع کمی شکننده است، زیرا اگر کد خود را بازسازی (refactor) کنیم (برای مثال نام فایل را تغییر دهیم یا آن را به مکانی دیگر منتقل کنیم)، تمام بخش‌هایی که شامل <code>patch</code> هستند باید به‌روزرسانی شوند، در غیر این صورت، تست از کار خواهد افتاد.\n",
    "    </p>\n",
    "    <p>\n",
    "        در مثال ارائه شده، این واقعیت که متد <code>notify()</code> به طور مستقیم به یک جزئیات پیاده‌سازی (ماژول <code>requests</code>) وابسته است، یک مسئله طراحی محسوب می‌شود؛ و این موضوع بر تست‌های واحد نیز تأثیر منفی دارد، همراه با شکنندگی ذکر شده.\n",
    "    </p>\n",
    "    <p>\n",
    "        ما هنوز هم نیاز داریم که آن متدها را با دوبل‌ها (mocks) جایگزین کنیم، اما اگر کد را بازسازی کنیم، می‌توانیم این کار را به شکل بهتری انجام دهیم. بیایید این متدها را به متدهای کوچک‌تر تقسیم کنیم و از همه مهم‌تر، وابستگی‌ها را تزریق کنیم به جای اینکه ثابت نگه داریم.\n",
    "    </p>\n",
    "    <p>\n",
    "        این کد اکنون اصل <strong>Dependency Inversion</strong> (وارونگی وابستگی) را اعمال می‌کند و انتظار دارد که با چیزی کار کند که از یک رابط (interface) پشتیبانی کند (در این مثال، یک رابط ضمنی)، مانند آنچه که ماژول <code>requests</code> ارائه می‌دهد.\n",
    "    </p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from constants import STATUS_ENDPOINT\n",
    "\n",
    "class BuildStatus:\n",
    "    endpoint = STATUS_ENDPOINT\n",
    "\n",
    "    def __init__(self, transport):\n",
    "        self.transport = transport\n",
    "\n",
    "    @staticmethod\n",
    "    def build_date() -> str:\n",
    "        return datetime.utcnow().isoformat()\n",
    "\n",
    "    def compose_payload(self, merge_request_id, status) -> dict:\n",
    "        return {\n",
    "            \"id\": merge_request_id,\n",
    "            \"status\": status,\n",
    "            \"built_at\": self.build_date(),\n",
    "        }\n",
    "\n",
    "    def deliver(self, payload):\n",
    "        response = self.transport.post(self.endpoint, json=payload)\n",
    "        response.raise_for_status()\n",
    "        return response\n",
    "\n",
    "    def notify(self, merge_request_id, status):\n",
    "        return self.deliver(self.compose_payload(merge_request_id, status))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir=\"rtl\" style=\"text-align: right; font-family: Arial, sans-serif; line-height: 1.6;\">\n",
    "    <p>\n",
    "        ما متدها را جدا می‌کنیم (توجه کنید که چگونه <code>notify</code> اکنون به دو متد <code>compose</code> و <code>deliver</code> تبدیل شده است)، متد <code>compose_payload()</code> را به یک متد جدید تبدیل می‌کنیم (به طوری که بتوانیم بدون نیاز به patch کردن کلاس، آن را جایگزین کنیم)، و وابستگی به بخش انتقال (<code>transport</code>) را تزریق می‌کنیم. حالا که <code>transport</code> به عنوان یک وابستگی تعریف شده است، تغییر این شیء به هر نوع دوبل (mock یا test double) که بخواهیم بسیار آسان‌تر می‌شود.\n",
    "    </p>\n",
    "    <p>\n",
    "        حتی امکان دارد که یک <strong>fixture</strong> از این شیء ارائه کنیم، به طوری که دوبل‌ها بر اساس نیاز جایگزین شوند.\n",
    "    </p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### If you want run this test, please open file \".\\refactoring\\test_refactoring_2.py\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@pytest.fixture\n",
    "def build_status():\n",
    "    bstatus = BuildStatus(Mock())\n",
    "    bstatus.build_date = Mock(return_value=\"2018-01-01T00:00:01\")\n",
    "    return bstatus\n",
    "\n",
    "def test_build_notification_sent(build_status):\n",
    "    build_status.notify(1234, \"OK\")\n",
    "    expected_payload = {\n",
    "        \"id\": 1234,\n",
    "        \"status\": \"OK\",\n",
    "        \"built_at\": build_status.build_date(),\n",
    "    }\n",
    "    build_status.transport.post.assert_called_with(\n",
    "        build_status.endpoint, json=expected_payload\n",
    "    )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
